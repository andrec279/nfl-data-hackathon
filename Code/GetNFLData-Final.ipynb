{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, array_contains, rand, rank, col\n",
    "import time as tm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from pyspark.sql.window import Window\n",
    "from nfldatagen import prep_nfl_data\n",
    "\n",
    "# Set up s3 bucket accessor and Spark Session\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(\"nyg-hackathon-949955964069\")\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark SQL For NGS Dataset\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.3\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note to Reader:\n",
    "The purpose of this notebook is to do the entire ETL process for the data I intend to use to build features and eventually a model off of, as a lot of data wrangling and preparation is needed on the NFL tracking data. After some basic raw data fetching from S3, batch ETL job is outsourced to a function I wrote called prep_nfl_data, which performs the entire job end-to-end with a given NFL team, and can therefore be looped through all 32 NFL teams get all data needed for downstream analysis. Details of this function are shown below:\n",
    "\n",
    "#### Data Extraction and Cleaning\n",
    "Select plays that are identified as high quality (playstate = approved, contains the necessary level of detail needed, including whether the play was a passing play) and use their playid's to pull the corresponding tracking data in raw JSON from S3. Several criteria for how the data being fed into the analysis must be organized:\n",
    "- Data is from regular season\n",
    "- Data includes passing plays only\n",
    "- Data must contain game and play id for use in iterative analyses later on\n",
    "- Data must include quarter - we will use this to extract one play per quarter for performance reasons (full scale implementation would use all the data)\n",
    "- Data includes line of scrimmage (LOS) - this can be used relative to player positions to orient the field direction for each play\n",
    "- Data includes ballsnaptime - this will be used to join context data to player tracking data and filter the play to the relevant time frame\n",
    "\n",
    "We are only interested in capturing blocking wins for passing plays, as the mechanisms for pass blocking vs run blocking are too different to apply one analytical method for.\n",
    "\n",
    "Overall extraction strategy is to find all the plays in the 2019 season (including pre and post) that match the criteria above, then use the game and play id of each play to extract the relevant tracking data directly from S3 filestore. I do this for each team in iteration\n",
    "\n",
    "#### Data Trimming and Wrangling: \n",
    "Main challenges with the data:\n",
    "- Size (hundreds of GB)\n",
    "- Complexity - JSON format, tracking data is nested several layers deep\n",
    "\n",
    "Solutions:\n",
    "- Import data with Spark - better for massive datasets\n",
    "- Filter out unnecessary data components (i.e. players not on field) and reduce uniformly across dataset for faster code performance \n",
    "- Use Spark explode to reshape data from raw JSON format and make more analytics-friendly\n",
    "- Push to Pandas DataFrame for easier manipulation and calculations\n",
    "\n",
    "#### Data Analysis\n",
    "Determine player assignments and calculate how often an o-lineman maintains his block successfully\n",
    "Output is block win rate on passing plays, accept only values with over 40 observations as valid\n",
    "\n",
    "Method:\n",
    "- Create a pandas DataFrame to store the output of the algorithm - pass block win rates for each player on the team in question\n",
    "- Iterate through plays identified via context df. Context df pulls all games from the 2019 season, filters to one random play per quarter of each game, then takes a random set of 60 plays from that data to ensure random sampling.\n",
    "- For each play:\n",
    "    - Determine field orientation - whether the play moves to the left or right. This determines how we determine which pass rushers are relevant to the play, as we define them as defenders that cross the LOS\n",
    "    - Walk through each play by timestep and calculate distances between each o-lineman and all pass rushers - o * d observations where o = number of o-lineman and d = number of pass rushers\n",
    "    - At each time step, define coverage by closest defender to each o-lineman. This allows for coverage to shift over time\n",
    "    - o-lineman is considered to be beaten by their pass rusher assignment if they are closer to the QB than the o-lineman and within 4 yards (estimated distance to apply pressure)\n",
    "    - pass block win = 1 play where an o-lineman was not beaten by his assignment\n",
    "- Tally total observations for each o-lineman and divide wins by total observations to get pass block win rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all playlist and event data into Spark Dataframe\n",
    "playlistdf_raw = spark.read.json('s3a://nyg-hackathon-949955964069/source_file/nfl/plays_playlist_game/year=2019/*.json')\n",
    "eventsdf_raw = spark.read.json('s3a://nyg-hackathon-949955964069/source_file/nfl/game_events/year=2019/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter exploded playlist Spark Dataframe to approved (for data quality) playstate and passing plays\n",
    "plist_e1 = playlistdf_raw.select(playlistdf_raw.gameId, explode(playlistdf_raw.plays).alias('play'))\n",
    "\n",
    "# Explode events Spark Dataframe and select only the playid and snaptime of the play\n",
    "elist_e1 = eventsdf_raw.select(explode(eventsdf_raw.events).alias('playevents'))\n",
    "elist_e2 = elist_e1.select(elist_e1.playevents.gameId.alias('gameid'), elist_e1.playevents.playId.alias('playid'), elist_e1.playevents.yardlineNumber.alias('los'), elist_e1.playevents.yardlineSide.alias('losside'), elist_e1.playevents.quarter.alias('qtr'), explode(elist_e1.playevents.events).alias('eventlist'))\n",
    "elist_e3 = elist_e2.select(elist_e2['gameid'], elist_e2['playid'], elist_e2['los'],elist_e2['losside'], elist_e2['qtr'], elist_e2['eventlist.name'].alias('eventtype'), elist_e2['eventlist.time'].alias('time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of NFL teams\n",
    "teams = ['TEN', 'BAL', 'MIN', 'NO', 'TB', 'SF', 'CAR', 'NE', 'KC', 'LA', \n",
    "        'LAC', 'SEA', 'OAK', 'BUF', 'CLE', 'CIN', 'HOU', 'DEN', 'CHI', 'IND',\n",
    "        'NYJ', 'MIA', 'ARI', 'DAL', 'ATL', 'JAX', 'PHI', 'WAS', 'DET', 'PIT',\n",
    "        'NYG', 'GB']\n",
    "time1 = tm.perf_counter()\n",
    "\n",
    "# Perform large batch NFL data ETL job (NOTE: approx 4 hour runtime)\n",
    "for team in teams:\n",
    "    _ = prep_nfl_data(plist_e1, elist_e3, team)\n",
    "    \n",
    "time2 = tm.perf_counter()\n",
    "print('Runtime: ', (time2 - time1)/60/60, 'hr')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
